{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "running-mississippi",
   "metadata": {},
   "source": [
    "# Setting up Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "governing-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Relevant Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "running-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Datasets\n",
    "video_comments_df = pd.read_csv(\"/home/chigoz/Downloads/Disso/00. Inputs/Video_Comments_Table.csv\", delimiter = \";\")\n",
    "text_df = pd.read_csv(\"/home/chigoz/Downloads/Disso/00. Inputs/text_data.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-freeware",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedicated-jumping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>video_title</th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_tags</th>\n",
       "      <th>video_comments</th>\n",
       "      <th>kw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3Cg8-eoNXdY</td>\n",
       "      <td>SLIM FIT DENIM JEANS LATEST PICK UPS + COLLECT...</td>\n",
       "      <td>This video has been sponsored by @FarFetch\\nGE...</td>\n",
       "      <td>['mens fashion', 'Denim Jeans', 'Slim fit deni...</td>\n",
       "      <td>['GET AN EXTRA 15% OFF SALE ITEMS  @FARFETCH  ...</td>\n",
       "      <td>slim fit denim jeans latest pick ups collectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zgXoCGUcR9w</td>\n",
       "      <td>‚òÄÔ∏è‚òÄÔ∏èSUMMER OUTFIT PICKUPS UNBOXING | TMONCLER,...</td>\n",
       "      <td>THIS VIDEO HAS BEEN SPONSORED BY @SEVEN STORE\\...</td>\n",
       "      <td>['mens fashion', 'MENS SUMMER OUTFITS', 'MENS ...</td>\n",
       "      <td>[\"GET 15% OFF WITH CODE 'THT15' WORKS ON NEW S...</td>\n",
       "      <td>summer outfit pickups unboxing tmoncler cdg n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFpME2E2GAY</td>\n",
       "      <td>MONTHLY DESIGNER SELECTIONS UNBOXING &amp; TRY-ON ...</td>\n",
       "      <td>THIS VIDEO HAS BEEN SPONSORED BY LUISAVIAROMA\\...</td>\n",
       "      <td>['mens fashion', 'tkmaxx outfit challenge', 's...</td>\n",
       "      <td>['I like that orange moncler jacket!', \"My fav...</td>\n",
       "      <td>monthly designer selections unboxing try on ft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4TJCzY3Ph88</td>\n",
       "      <td>ULTIMATE BALENCIAGA TRIPLE S , TRACK &amp; SPEED S...</td>\n",
       "      <td>This video has been sponsored @Browns \\n@Brown...</td>\n",
       "      <td>['mens fashion', 'tkmaxx outfit challenge', 's...</td>\n",
       "      <td>['ENTER OUR 250 COMPETITION GIVEAWAY TO SPEND ...</td>\n",
       "      <td>ultimate balenciaga triple s track speed sock ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX3TFXp2y2M</td>\n",
       "      <td>üî•üî• 6 HOT MENSWEAR SUMMER SALES HAPPENING RIGHT...</td>\n",
       "      <td>https://thehoxtontrend.com/latest-spring-sales...</td>\n",
       "      <td>['mens fashion', 'tkmaxx outfit challenge', 's...</td>\n",
       "      <td>['Reppin the Patta collab! ', 'Thanks picking ...</td>\n",
       "      <td>hot menswear summer sales happening right now...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                        video_title  \\\n",
       "0  3Cg8-eoNXdY  SLIM FIT DENIM JEANS LATEST PICK UPS + COLLECT...   \n",
       "1  zgXoCGUcR9w  ‚òÄÔ∏è‚òÄÔ∏èSUMMER OUTFIT PICKUPS UNBOXING | TMONCLER,...   \n",
       "2  AFpME2E2GAY  MONTHLY DESIGNER SELECTIONS UNBOXING & TRY-ON ...   \n",
       "3  4TJCzY3Ph88  ULTIMATE BALENCIAGA TRIPLE S , TRACK & SPEED S...   \n",
       "4  TX3TFXp2y2M  üî•üî• 6 HOT MENSWEAR SUMMER SALES HAPPENING RIGHT...   \n",
       "\n",
       "                                   video_description  \\\n",
       "0  This video has been sponsored by @FarFetch\\nGE...   \n",
       "1  THIS VIDEO HAS BEEN SPONSORED BY @SEVEN STORE\\...   \n",
       "2  THIS VIDEO HAS BEEN SPONSORED BY LUISAVIAROMA\\...   \n",
       "3  This video has been sponsored @Browns \\n@Brown...   \n",
       "4  https://thehoxtontrend.com/latest-spring-sales...   \n",
       "\n",
       "                                          video_tags  \\\n",
       "0  ['mens fashion', 'Denim Jeans', 'Slim fit deni...   \n",
       "1  ['mens fashion', 'MENS SUMMER OUTFITS', 'MENS ...   \n",
       "2  ['mens fashion', 'tkmaxx outfit challenge', 's...   \n",
       "3  ['mens fashion', 'tkmaxx outfit challenge', 's...   \n",
       "4  ['mens fashion', 'tkmaxx outfit challenge', 's...   \n",
       "\n",
       "                                      video_comments  \\\n",
       "0  ['GET AN EXTRA 15% OFF SALE ITEMS  @FARFETCH  ...   \n",
       "1  [\"GET 15% OFF WITH CODE 'THT15' WORKS ON NEW S...   \n",
       "2  ['I like that orange moncler jacket!', \"My fav...   \n",
       "3  ['ENTER OUR 250 COMPETITION GIVEAWAY TO SPEND ...   \n",
       "4  ['Reppin the Patta collab! ', 'Thanks picking ...   \n",
       "\n",
       "                                             kw_text  \n",
       "0  slim fit denim jeans latest pick ups collectio...  \n",
       "1   summer outfit pickups unboxing tmoncler cdg n...  \n",
       "2  monthly designer selections unboxing try on ft...  \n",
       "3  ultimate balenciaga triple s track speed sock ...  \n",
       "4   hot menswear summer sales happening right now...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining function to preprocess text data\n",
    "def pre_process(text):\n",
    "    \n",
    "    # convert all characters to lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove urls\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Combining the video title and description fields \n",
    "text_df['kw_text'] = text_df['video_title'] + text_df['video_description']\n",
    "\n",
    "# Applying the pre processing to the combined text field\n",
    "text_df['kw_text'] = text_df['kw_text'].apply(lambda x:pre_process(x))\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "located-cowboy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chigoz/anaconda3/envs/ml_module/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['articl', 'mon'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# Setting lists of stop words\n",
    "stopwords=[\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\", \"year\", \"day\", \"month\", \"yesterday\"]\n",
    "\n",
    "#get the text column \n",
    "docs=text_df['kw_text'].tolist()\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.85,\n",
    "                   stop_words=stopwords)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "three-moral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 1772)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows there is 210 documents (video titles/descriptions) and vocab of 2118 after the countvectoriser\n",
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vertical-richardson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting parameters and fitting tf idf transformer\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spanish-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function which perserves the column index while soting the vector values\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "# Creating a function to get the feature value names and scores\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    \n",
    "    return feature_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "armed-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning column used for keyword extraction to the doc variable\n",
    "doc=text_df['kw_text']\n",
    "\n",
    "# Getting feature names from matrix\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "#generate tf-idf for all documents in your list. docs_test has 500 documents\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform(doc))\n",
    "\n",
    "results=[]\n",
    "for i in range(tf_idf_vector.shape[0]):\n",
    "    \n",
    "    # get vector for a single document\n",
    "    curr_vector=tf_idf_vector[i]\n",
    "    \n",
    "    #sort the tf-idf vector by descending order of scores\n",
    "    sorted_items=sort_coo(curr_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "    \n",
    "    # appending keywords to keyword list\n",
    "    results.append(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ancient-namibia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>accounts</th>\n",
       "      <th>adidas</th>\n",
       "      <th>ages</th>\n",
       "      <th>air</th>\n",
       "      <th>alexander</th>\n",
       "      <th>alligator</th>\n",
       "      <th>alpha</th>\n",
       "      <th>alternatives</th>\n",
       "      <th>alyx</th>\n",
       "      <th>...</th>\n",
       "      <th>winner</th>\n",
       "      <th>winter</th>\n",
       "      <th>wnjoy</th>\n",
       "      <th>wol</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>worth</th>\n",
       "      <th>wotherspoon</th>\n",
       "      <th>wyndham</th>\n",
       "      <th>yeezy</th>\n",
       "      <th>zee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3Cg8-eoNXdY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zgXoCGUcR9w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFpME2E2GAY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4TJCzY3Ph88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX3TFXp2y2M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 577 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  accounts  adidas  ages  air  alexander  alligator  alpha  \\\n",
       "0  3Cg8-eoNXdY         0       0     0    0          0          0      0   \n",
       "1  zgXoCGUcR9w         0       0     0    0          0          0      0   \n",
       "2  AFpME2E2GAY         0       0     0    0          0          0      0   \n",
       "3  4TJCzY3Ph88         0       0     0    0          0          0      0   \n",
       "4  TX3TFXp2y2M         0       0     0    0          0          0      0   \n",
       "\n",
       "   alternatives  alyx  ...  winner  winter  wnjoy  wol  worldwide  worth  \\\n",
       "0             0     0  ...       0       0      0    0          0      0   \n",
       "1             0     0  ...       0       0      0    0          0      0   \n",
       "2             0     0  ...       0       0      0    0          0      0   \n",
       "3             0     0  ...       0       0      0    0          0      0   \n",
       "4             0     0  ...       0       0      0    0          0      0   \n",
       "\n",
       "   wotherspoon  wyndham  yeezy  zee  \n",
       "0            0        0      0    0  \n",
       "1            0        0      0    0  \n",
       "2            0        0      0    0  \n",
       "3            0        0      0    0  \n",
       "4            0        0      0    0  \n",
       "\n",
       "[5 rows x 577 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a keyword results dataframe\n",
    "results_df = pd.DataFrame(results, columns=[\"keyword_1\", \"keyword_2\", \"keyword_3\", \"keyword_4\", \"keyword_5\"])\n",
    "\n",
    "# Creating dummy variables for each keyword\n",
    "results_df = results_df.stack().str.get_dummies().sum(level=0)\n",
    "results_df\n",
    "\n",
    "# Appending results of a keyword dataframe to an overall NLP dataframe\n",
    "nlp_df = pd.concat([text_df[\"video_id\"], results_df], axis=1)\n",
    "nlp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-granny",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ranking-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to preprocess text data\n",
    "def pre_process(text):\n",
    "    \n",
    "    # Remove Non-Letters\n",
    "    # (Search for non letters and replace with spaces)\n",
    "    text=re.sub(\"[^a-zA-Z]\",\" \",str(text))\n",
    "    \n",
    "    #remove urls\n",
    "    text=re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Preprocessing comments\n",
    "all_comments = list(video_comments_df[\"comment\"].apply(lambda x:pre_process(x)))\n",
    "\n",
    "# Empty list for each comment's sentiment score\n",
    "sentiment_scores = []\n",
    "\n",
    "# Setting up sentiment analyser function\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Creating a list containing all the levels within the comments column of the comments dataframe\n",
    "levels = list(range(0, len(video_comments_df)))\n",
    "\n",
    "# For loop to analyse all the comments \n",
    "for comment in levels:\n",
    "    \n",
    "    score = sia.polarity_scores(all_comments[comment])[\"compound\"]\n",
    "    \n",
    "    sentiment_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aerial-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding sentiment score column to the comments dataframe\n",
    "video_comments_df[\"sentiment_score\"] = sentiment_scores\n",
    "\n",
    "# Calculating the mean sentiment score for each video\n",
    "avg_sentiment_scores = video_comments_df[['video_id', 'sentiment_score']].groupby('video_id').mean()\n",
    "avg_sentiment_scores.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "upset-proxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>accounts</th>\n",
       "      <th>adidas</th>\n",
       "      <th>ages</th>\n",
       "      <th>air</th>\n",
       "      <th>alexander</th>\n",
       "      <th>alligator</th>\n",
       "      <th>alpha</th>\n",
       "      <th>alternatives</th>\n",
       "      <th>alyx</th>\n",
       "      <th>...</th>\n",
       "      <th>winter</th>\n",
       "      <th>wnjoy</th>\n",
       "      <th>wol</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>worth</th>\n",
       "      <th>wotherspoon</th>\n",
       "      <th>wyndham</th>\n",
       "      <th>yeezy</th>\n",
       "      <th>zee</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3Cg8-eoNXdY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.358728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zgXoCGUcR9w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.211008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFpME2E2GAY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.325584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4TJCzY3Ph88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.213441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TX3TFXp2y2M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.167957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id  accounts  adidas  ages  air  alexander  alligator  alpha  \\\n",
       "0  3Cg8-eoNXdY         0       0     0    0          0          0      0   \n",
       "1  zgXoCGUcR9w         0       0     0    0          0          0      0   \n",
       "2  AFpME2E2GAY         0       0     0    0          0          0      0   \n",
       "3  4TJCzY3Ph88         0       0     0    0          0          0      0   \n",
       "4  TX3TFXp2y2M         0       0     0    0          0          0      0   \n",
       "\n",
       "   alternatives  alyx  ...  winter  wnjoy  wol  worldwide  worth  wotherspoon  \\\n",
       "0             0     0  ...       0      0    0          0      0            0   \n",
       "1             0     0  ...       0      0    0          0      0            0   \n",
       "2             0     0  ...       0      0    0          0      0            0   \n",
       "3             0     0  ...       0      0    0          0      0            0   \n",
       "4             0     0  ...       0      0    0          0      0            0   \n",
       "\n",
       "   wyndham  yeezy  zee  sentiment_score  \n",
       "0        0      0    0         0.358728  \n",
       "1        0      0    0         0.211008  \n",
       "2        0      0    0         0.325584  \n",
       "3        0      0    0         0.213441  \n",
       "4        0      0    0         0.167957  \n",
       "\n",
       "[5 rows x 578 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging the sentiment scores onto the nlp data dataframe\n",
    "nlp_df = nlp_df.merge(avg_sentiment_scores, on = \"video_id\")\n",
    "nlp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "awful-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Dataframe to CSV File\n",
    "nlp_df.to_csv(\"/home/chigoz/Downloads/Disso/00. Inputs/nlp_data.csv\", index=False, sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
